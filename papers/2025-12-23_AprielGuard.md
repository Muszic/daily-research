# AprielGuard

- **Category:** NLP
- **Published:** 2025-12-23
- **Source:** [Original ArXiv Link](http://arxiv.org/abs/2512.20293v1)

---

## üßê Problem

The increasing deployment of Large Language Models (LLMs) in conversational and agentic settings necessitates robust safeguards against unsafe and adversarial behaviors. Existing moderation tools exhibit several limitations:

*   **Fragmented Approach:** They often treat safety risks (e.g., toxicity, bias) and adversarial threats (e.g., prompt injections, jailbreaks) as separate problems, requiring distinct models or inference calls. This separation limits robustness, generalizability, and increases deployment complexity and latency.
*   **Inadequate for Agentic AI:** Traditional moderation systems, designed for short-form, single-turn interactions, are ill-equipped to detect complex, multi-step threats arising from autonomous agentic workflows. These include indirect prompt injections, multi-turn jailbreaks, and long-context manipulations that exploit an LLM's reasoning or planning mechanisms. State-of-the-art models are highly vulnerable to such agentic exploits.
*   **Lack of Interpretability:** Most current safeguard systems offer little insight into *why* a moderation decision was made, hindering compliance, auditing, and user trust.
*   **Licensing and Data Constraints:** Many existing models operate under non-permissive licenses or rely on synthetically generated data from closed/commercial models, limiting their applicability in broader research and commercial contexts.

## üõ†Ô∏è Method

AprielGuard introduces an 8B parameter safeguard model designed to unify the detection of safety risks and adversarial attacks.

*   **Unified Moderation Framework:** It integrates safety moderation and adversarial defense under a single, comprehensive taxonomy and learning framework. This allows for consistent classification across various categories like toxicity, bias, prompt injection, and jailbreaks, including novel Agentic Jailbreaks.
*   **Robust and Diverse Training Data:** The model is trained on a unique, entirely synthetic dataset covering:
    *   **Interaction Modalities:** Standalone prompts, multi-turn conversations, and complex agentic workflows (multi-step interactions involving tool calls, APIs, or code execution).
    *   **Content Types:** Balanced representation of safe/unsafe and adversarial/non-adversarial samples.
    *   **Generation & Augmentation:** Data is generated using models like Mixtral-8x7B and internal uncensored models (via SyGra and NVIDIA Nemo Curator), filtered for semantic and syntactic redundancy, and augmented with character-level noise, typos, leetspeak, paraphrasing, and syntactic reordering to enhance robustness against real-world variations and adversarial manipulations.
*   **Structured Reasoning Traces:** Each data point is augmented with structured reasoning annotations, generated by LLMs explaining the rationale behind classification decisions. This enables AprielGuard to produce transparent, category-aware explanations, improving interpretability.
*   **Unified Taxonomy:** It adopts a 16-task taxonomy (from SALAD Data) for safety risks and introduces a custom taxonomy for adversarial attacks. Each input is independently assessed for both dimensions.
*   **Model Training:** AprielGuard is developed through supervised fine-tuning of a downscaled variant of the Apriel-1.5‚Äì15B-Thinker model. It uses a specific chat template format that encapsulates content within a `<|content|>` tag and provides moderation instructions via the `user` role. The model is trained for 3 epochs with a batch size of 1 (gradient accumulation over 8 steps) and Adam optimizer.
*   **Flexible Output Format:** AprielGuard can generate outputs in a predefined structured format, either with or without detailed reasoning. It intelligently assesses safety risks in assistant responses and adversarial behavior in user messages for conversational content, and both for standalone prompts.

## üìä Impact

AprielGuard demonstrates significant advancements in LLM safeguarding:

*   **Superior Performance:** It achieves strong performance in detecting harmful content and adversarial manipulations, outperforming existing open-source guardrails like Llama-Guard and Granite Guardian, particularly in multi-step and reasoning-intensive scenarios.
*   **Agentic AI Protection:** Crucially, it effectively identifies threats emerging from reasoning chains, planning sequences, and tool-use decisions in agentic workflows, a critical gap where state-of-the-art LLMs have shown high vulnerability. It is the only model among compared baselines to support Agentic Jailbreak detection.
*   **Enhanced Interpretability:** By leveraging structured reasoning, AprielGuard provides transparent explanations for its moderation decisions, fostering trust and aiding compliance and auditing processes.
*   **Unified and Efficient:** Its unified framework for safety and adversarial detection simplifies deployment and reduces latency compared to systems requiring separate models or inference calls.
*   **Accessibility:** As a compact (8B parameters) and permissively licensed model, AprielGuard is easily deployable in real-world workflows and aims to advance transparent and reproducible research in LLM safeguards.
