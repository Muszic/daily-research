# T-pro 2.0: An Efficient Russian Hybrid-Reasoning Model and Playground

- **Category:** NLP
- **Published:** 2025-12-11
- **Source:** [Original ArXiv Link](http://arxiv.org/abs/2512.10430v1)

---

## üßê Problem Statement
The paper addresses several gaps in the Russian open-source Large Language Model (LLM) ecosystem:
*   **Limited Progress in Open-Source Russian LLMs:** Most high-performing Russian models are proprietary (closed-source APIs), while open models are often small adaptations of multilingual systems with limited capabilities.
*   **Lack of a Unified Ecosystem for Russian Reasoning:** There is a scarcity of high-quality evaluation sets specifically for Russian-language reasoning.
*   **Absence of Interactive Tools:** Few public demos exist to compare direct answering and step-by-step reasoning, inspect inference-time optimizations, or observe the impact of decoding speed on user experience for Russian LLMs.
*   **Inefficient Tokenization for Cyrillic Languages:** Multilingual models typically suffer from systematic under-tokenization of Russian and other Cyrillic languages, leading to inefficiencies.

## üõ†Ô∏è Methodology
The authors introduce **T-pro 2.0**, an open-weight Russian LLM designed for hybrid reasoning and efficient inference, along with a comprehensive ecosystem of resources.

Key methodological components include:
*   **Cyrillic-dense Tokenizer:** Adapted from Qwen3's vocabulary, 34k low-frequency non-Cyrillic tokens were replaced with high-frequency Cyrillic tokens, improving compression efficiency for Russian and other Cyrillic languages.
*   **Hybrid Reasoning Capability:** The model supports two complementary modes: direct answering and explicit reasoning traces, allowing applications to balance speed and accuracy.
*   **Training Pipeline:**
    *   **Instructional Midtraining:** The Qwen3-32B model was adapted to the new tokenizer and enhanced for reasoning using 40B tokens from curated open-source instructions, synthetic tasks, and parallel corpora (49% Russian, 36% English, with strong emphasis on Reasoning, General QA, and Math). Responses were regenerated using a Qwen3-235B-A22B teacher model.
    *   **Reward Model (RM) Construction:** A dedicated RM, initialized from Qwen3-32B, was trained using a Bradley‚ÄìTerry preference objective on synthetically generated preference data from knockdown tournaments of model completions.
    *   **General Post-Training (SFT):** A multi-stage filtering pipeline reduced ~14M raw instructions to 468k high-quality, domain-balanced (Math, Code, Science, General Instruction, General Knowledge, Writing) and difficulty-tiered (School, Student, Professor) samples. Candidate completions were generated by Qwen3-235B-A22B or DeepSeek-V3 teachers and filtered via the RM.
    *   **Reasoning SFT:** ~30K reasoning-intensive samples (translated from English covering math, science, code) were processed, with solutions generated by teacher models and filtered via RM-based rejection.
    *   **Direct Preference Optimization (DPO):** Applied to 100k instructions from the T-Wix dataset (90/10 general-to-reasoning ratio), using RM-scored on-policy completions to form high-contrast preference pairs (best vs. worst).
*   **Efficient Inference:** An adapted **EAGLE-style speculative decoding pipeline** was integrated. A lightweight draft model (single Llama-2-based decoder layer with FR-Spec component) proposes candidate tokens, which are then verified by the frozen 32B target model, accelerated via SGLang.
*   **Released Resources:**
    *   **T-pro 2.0 model weights**
    *   **T-Wix500k instruction corpus:** The largest open Russian hybrid-reasoning SFT dataset (~500k samples).
    *   **T-Math reasoning benchmark:** A new benchmark of 331 challenging high-school olympiad-level mathematics problems in Russian.
    *   **EAGLE weights.**
*   **Interactive Web Demo:** A public, stateless web application (http://t-pro-2-0.streamlit.app) allows users to interact with T-pro 2.0, compare reasoning and non-reasoning modes, and observe inference speedups (telemetry includes generated tokens, latency, throughput, and speculative token acceptance ratio). It runs on serverless SGLang endpoints with NVIDIA H100 GPUs, comparing T-pro 2.0 with EAGLE against a Qwen3-32B baseline with standard autoregressive decoding.

## üìä Results & Impact
T-pro 2.0 demonstrates strong performance and significant contributions to the Russian LLM landscape:

*   **Tokenizer Efficiency:** The Cyrillic-dense tokenizer dramatically improves tokenization for Russian, increasing the share of words tokenized into at most two tokens on Russian Wikipedia from 38% to 60%. It also shows consistently shorter average segmentations across eight Cyrillic languages compared to other models.
*   **Inference Speedup:** The EAGLE-based speculative decoding pipeline achieves an average speedup of **1.85√ó** in standard mode and **1.83√ó** in reasoning mode (at temperature 0.8), with STEM domains showing higher acceleration (1.99√ó vs. 1.62√ó for humanities).
*   **Superior General Knowledge & Dialogue Abilities:** T-pro 2.0 performs consistently well on Russian benchmarks, scoring 0.66 on MERA and 0.697 on ruMMLU-Pro (comparable to GPT-4o). It **outperforms all open-source systems and most proprietary ones** on dialogue tasks, achieving 91.1 on Arena Hard Ru and 72.6 on WildChat Hard Ru.
*   **State-of-the-Art Russian Reasoning:**
    *   On the new **T-Math benchmark**, it scores 0.541, demonstrating strong capabilities on challenging olympiad-style problems.
    *   On ruAIME 2024/2025, it achieves 0.704 / 0.646, **sharply outperforming DeepSeek-V3, GPT-4o, and all proprietary Russian models**.
    *   It also shows strong results on ruMATH-500 (0.94) and Vikhr Math (0.799).
*   **Bilingual Competence Preservation:** Despite intense Russian-focused training, T-pro 2.0 maintains competitive performance on **English reasoning benchmarks** (e.g., 0.765 on AIME 2024, 0.966 on MATH-500), showing no notable degradation.
*   **Open-Source Ecosystem & Reproducibility:** The release of T-pro 2.0 weights, the T-Wix dataset, the T-Math benchmark, and EAGLE weights under permissive licenses (Apache-2.0, ODC-By) provides an accessible open system for building, evaluating, and extending efficient Russian LLM applications and fosters research in Russian-language reasoning.
*   **Practical Impact:** The work highlights that targeted adaptation of multilingual backbones, combined with careful tokenizer design and inference optimization, is a practical and reproducible route for building high-quality, efficient, and reasoning-capable LLMs for languages with limited resources beyond English.

---
*Generated by Gemini 2.5 Flash*
