# Consistency Meets Verification: Enhancing Test Generation Quality in Large Language Models Without Ground-Truth Solutions

- **Category:** Software Engineering
- **Date:** 2026-02-11
- **Link:** http://arxiv.org/abs/2602.10522v1

---
Here's a summary of the research paper "Consistency Meets Verification: Enhancing Test Generation Quality in Large Language Models Without Ground-Truth Solutions" in Markdown format:

## Problem

Large Language Models (LLMs) show promise in automated test generation, but face significant challenges in verifying the correctness of generated test cases. Existing methods typically rely on **ground-truth code implementations** for verification, which creates a "circularity of error":
*   **Bug Propagation Risk:** Tests are generated to match existing (potentially faulty) code logic, rather than intended specifications, making them ineffective at detecting actual bugs.
*   **Limited Applicability:** This approach is impossible in scenarios like Test-Driven Development (TDD) where the code under test does not yet exist.
*   **Hallucinations:** LLMs often generate invalid tests due to hallucinations, producing plausible but incorrect code.

The fundamental problem is how to generate and verify **reliable, high-quality test cases without relying on a pre-existing, trusted code implementation.**

## Method

The paper proposes **ConVerTest**, a novel two-stage pipeline for synthesizing reliable tests and code solutions without requiring ground-truth implementations. It integrates three core strategies: **Self-Consistency (SC)**, **Chain-of-Verification (CoVe)**, and a **Dual Execution Agreement**.

**ConVerTest operates in two stages:**

### Stage 1: Consistency-Driven Generation

This stage runs two concurrent pipelines for generating consistent tests and verified code candidates:

1.  **Test Generation via Self-Consistency (SC):**
    *   Employs a two-stage SC approach to generate diverse and consistent test cases.
    *   **Stage 1: Test Stub Generation:** The LLM generates `M` diverse test stubs (skeleton structures defining test scenarios without concrete expected outputs or assertions). This ensures broad coverage of test scenarios.
    *   **Stage 2: Completion and Refinement via SC:** For each stub, the LLM is prompted `N` times to generate complete test functions (including specific expected outputs and assertions). The most frequent (most consistent) completion, determined by majority vote via Abstract Syntax Tree (AST) analysis, is selected. This refines each stub into a single, highly consistent test case.

2.  **Code Verification via Chain-of-Verification (CoVe):**
    *   Uses an iterative refinement process to generate `Z` verified code candidate solutions.
    *   **Baseline Solution Generation:** An initial candidate solution is generated by the LLM from the problem statement.
    *   **Verification Plan Formulation:** The LLM generates a series of specific, analytical questions to probe the baseline code's correctness, logic, edge case handling, and constraint adherence.
    *   **Execution of Verification Questions:** The questions are systematically addressed, and detailed "verification answers" are produced, diagnosing issues.
    *   **Iterative Refinement (Verification-Guided Generation):** If issues are identified, the LLM is prompted to produce a revised solution, using the original prompt, flawed solution, questions, and answers as context. This process repeats until no issues are found or for `Z` iterations.

### Stage 2: Consensus Verification

This final stage uses the `M` SC-generated tests and `Z` CoVe-generated code candidates to mutually validate each other and filter invalid tests:

*   **Dual Execution Agreement:**
    *   Each of the `Z` candidate solutions is executed against every one of the `M` generated test cases, creating a pass/fail matrix.
    *   Solutions are grouped into "agreement sets" if they pass the exact same subset of tests.
    *   Each agreement set is assigned a **consensus score**: `(number of tests passed) × √(number of solutions in set)`.
    *   The solution from the highest-scoring agreement set is selected as the **final, most reliable code solution**. This "best solution" acts as a reliable proxy for ground-truth.
    *   **Test Validation:** Each generated test is then labeled as **valid** if it passes against this chosen "best solution" and **invalid** if it fails. Invalid tests are discarded, and valid ones form the final test suite.

## Impact

ConVerTest significantly enhances the quality and reliability of LLM-generated tests, particularly in the challenging scenario where no ground-truth code exists:

*   **Improved Test Validity:** Increases test validity rates by **up to 39%** over baselines.
*   **Enhanced Test Adequacy:** Improves line coverage by **up to 28%** and mutation scores by **up to 18%**.
*   **Effective Hallucination Mitigation:** The combination of consistency-driven generation and consensus-based verification effectively mitigates LLM hallucinations, leading to more trustworthy test suites.
*   **High Precision and Recall:** The consensus verification stage accurately classifies valid/invalid tests, achieving high precision (ensuring the final suite is clean) and recall (preserving most valid tests).
*   **Robustness Across Models:** While models vary in baseline performance, ConVerTest consistently provides substantial improvements for all tested LLMs (CodeQwen3, GPT-5-Mini, Gemma3.3).
*   **Component Contribution:** Ablation studies confirm that all components (Self-Consistency, Chain-of-Verification, and Two-Stage Generation) contribute positively to the overall performance, highlighting their complementary strengths.
*   **Enables Autonomous Software Testing:** Offers a robust solution for enhancing the reliability of autonomous software testing agents and supports scenarios like Test-Driven Development where code is not yet available.
*   **Reproducibility:** The authors have released data and source code to facilitate replication and extension.