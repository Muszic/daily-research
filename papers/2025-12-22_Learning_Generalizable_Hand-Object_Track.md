# Learning Generalizable Hand-Object Tracking from Synthetic Demonstrations

- **Category:** Robotics
- **Published:** 2025-12-22
- **Source:** [Original ArXiv Link](http://arxiv.org/abs/2512.19583v1)

---

## üßê Problem

Training generalizable hand-object tracking controllers for dexterous manipulation faces a fundamental data acquisition challenge. Traditional methods rely on human demonstrations, which are either:
*   **Labor-intensive** to collect (e.g., via motion capture).
*   **Difficult to control for quality and distribution** when estimated from internet videos, often struggling with occlusion and depth ambiguity.
*   **Limited in scalability** as they cannot generate novel manipulation patterns beyond existing reference data.

This data bottleneck severely constrains progress towards developing scalable foundation controllers for complex, dynamic hand-object interaction (HOI) tasks, which require human-like dexterity. Existing reinforcement learning approaches for dexterous manipulation often rely on task-specific reward engineering, further limiting their generalization.

## üõ†Ô∏è Method

The authors propose a system for learning generalizable hand-object tracking controllers purely from synthetic data, without requiring human demonstrations. The system consists of two key components:

1.  **HOP (Hand-Object Planner):** A scalable data synthesis framework that generates diverse hand-object trajectories for various object shapes and hand morphologies.
    *   **Grasp Pose Synthesis & Refinement:** It starts by generating numerous static grasp configurations using force-closure optimization and a pre-trained grasp generator. These candidates are then filtered and refined by a "coach policy" trained via reinforcement learning to ensure physical plausibility and stability in simulation.
    *   **Meta-Skill Trajectory Synthesis:** HOP uses these stable grasp poses as anchors to procedurally synthesize pseudo-demonstrations for eight fundamental meta-skills (e.g., Grasp, Place, Move, Rotate, Catch, Throw, Regrasp) and their combinations. These trajectories are created by interpolating randomly generated hand-object states and grasp configurations, ensuring smooth transitions between skills.

2.  **HOT (Hand-Object Tracker):** A tracking-based controller trained to track these complex, highly dynamic, and long-sequence manipulation trajectories generated by HOP.
    *   **Reinforcement Learning with Unified Imitation Reward:** HOT is trained in physical simulation using a unified Hand-Object Interaction (HOI) imitation reward, which multiplicatively combines hand, wrist, object, interaction, and contact tracking components. This reward enables learning despite physical implausibilities in synthetic data.
    *   **Policy Formulation:** A single MLP-based policy outputs residual adjustments for the wrist's 6-DoF pose and target finger joint rotations. Observations include current hand and object states, and a goal state for the next timestep, all localized relative to the wrist.
    *   **Two-Stage Teacher-Student Training:** To enhance scalability and learning efficiency across diverse skills and objects, specialized "teacher" policies are first trained for individual meta-skills/objects. A unified "student" policy then learns from these teachers, leveraging them for online data refinement and action guidance through behavior cloning, which aids in robustness to imperfect demonstrations.
    *   **Domain Randomization Curriculum:** To improve robustness, various aspects (object physical properties, initial states, perturbation forces) are randomized during training, with intensity gradually increasing over a curriculum. Adaptive sampling reweights challenging trajectories for more learning opportunities.

At inference, the system uses a modular pipeline: high-level directives (from LLMs, VLMs, generative models, or human data) are converted into sparse waypoints, which HOP expands into dense hand-object trajectories, and HOT then executes these trajectories in simulation.

## üìä Impact

The proposed system represents a significant step towards scalable foundation controllers for dexterous manipulation, effectively breaking the long-standing data bottleneck by learning entirely from synthetic data.

*   **Generalizability:** HOT, trained purely on synthetic data from HOP, demonstrates remarkable generalization across diverse hand and object morphologies, object shapes, and the entire operational space.
*   **Robust Tracking of Complex Tasks:** It robustly tracks challenging, long-horizon sequences, including object re-arrangement, agile in-hand reorientation, and composite tasks like Catch‚ÄìRotate‚ÄìMove and Grasp‚ÄìMove‚ÄìPlace‚ÄìRotate.
*   **Zero-Shot Transfer to Downstream Applications:** The system enables seamless zero-shot transfer to various downstream applications. When integrated with LLMs or VLMs, it can follow language instructions to control both data synthesis and tracking for goal-directed manipulation. It also effectively tracks real human demonstrations and data from HOI generative models.
*   **Overcoming Data Bottleneck:** By relying solely on procedurally generated synthetic data, the method eliminates the need for labor-intensive human demonstrations, offering superior scalability and flexibility, particularly in scenarios where real-world data collection is impractical or scarce.
*   **Improved Performance:** Quantitative evaluations show HOT significantly outperforms baseline open-loop controllers, as well as state-of-the-art HOI imitation methods that struggle with imperfect data and generalization.
